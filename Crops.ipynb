{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nbformat --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import resample\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('augmented_file_50x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features list\n",
    "features = ['N', 'P', 'K', 'temperature', 'ph', 'rainfall']\n",
    "\n",
    "# Compute skewness and kurtosis\n",
    "skewness = data[features].skew()\n",
    "kurtosis = data[features].kurt()\n",
    "\n",
    "# Create a new DataFrame to compile all statistics\n",
    "stats_df = pd.DataFrame({\n",
    "    'Mean': data[features].mean(),\n",
    "    'Std Dev': data[features].std(),\n",
    "    'Min': data[features].min(),\n",
    "    'Max': data[features].max(),\n",
    "    'Skewness': skewness,\n",
    "    'Kurtosis': kurtosis\n",
    "})\n",
    "\n",
    "# Round the statistics to desired decimal places\n",
    "stats_df = stats_df.round(2)\n",
    "\n",
    "# Replace -0.00 with 0.00 for cleanliness\n",
    "stats_df = stats_df.replace(-0.00, 0.00)\n",
    "\n",
    "print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data distribution between N, P, and K\n",
    "\n",
    "# Set the size for the overall figure\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plotting the histogram and KDE for N\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(data['N'], color=\"red\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of N')\n",
    "\n",
    "# Plotting the histogram and KDE for P\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(data['P'], color=\"blue\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of P')\n",
    "\n",
    "# Plotting the histogram and KDE for K\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(data['K'], color=\"green\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of K')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data distribution between temperature, ph and rainfall\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plotting the histogram and KDE for temperature\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(data['temperature'], color=\"red\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of Temperature')\n",
    "\n",
    "# Plotting the histogram and KDE for ph\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(data['ph'], color=\"blue\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of pH')\n",
    "\n",
    "# Plotting the histogram and KDE for rainfall\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(data['rainfall'], color=\"green\", bins=15, kde=True, alpha=0.2)\n",
    "plt.title('Distribution of Rainfall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic pairplot with enhancements\n",
    "# sns.pairplot(data,\n",
    "#              hue='label',\n",
    "#              palette='pastel',       # Use pastel palette for distinction and aesthetics\n",
    "#              height=2.5,             # Increase plot size\n",
    "#              aspect=1,               # Aspect ratio\n",
    "#              diag_kind='kde',        # KDE plots on the diagonal\n",
    "#              plot_kws={'alpha': 0.7, 's': 80, 'edgecolor': 'k'},  # Opacity, size, and edge color of markers\n",
    "#              diag_kws={'shade': True}   # Shade the KDE plots\n",
    "#              )\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = data[features].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Lists to collect split data\n",
    "X_train_list = []\n",
    "X_val_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_val_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# Split each label's data separately\n",
    "labels = y.unique()\n",
    "for label in labels:\n",
    "    X_label = X[y == label]\n",
    "    y_label = y[y == label]\n",
    "    \n",
    "    # First, separate out the training data (70%)\n",
    "    X_train_label, X_temp_label, y_train_label, y_temp_label = train_test_split(\n",
    "        X_label, y_label, test_size=0.30, random_state=0)\n",
    "    \n",
    "    # Now, split the remaining data into validation and test sets (20% test, 10% validation)\n",
    "    X_test_label, X_val_label, y_test_label, y_val_label = train_test_split(\n",
    "        X_temp_label, y_temp_label, test_size=0.3333, random_state=0)\n",
    "    \n",
    "    X_train_list.append(X_train_label)\n",
    "    X_val_list.append(X_val_label)\n",
    "    X_test_list.append(X_test_label)\n",
    "    y_train_list.append(y_train_label)\n",
    "    y_val_list.append(y_val_label)\n",
    "    y_test_list.append(y_test_label)\n",
    "\n",
    "# Concatenate splits\n",
    "X_train = pd.concat(X_train_list, axis=0)\n",
    "y_train = pd.concat(y_train_list, axis=0)\n",
    "X_val = pd.concat(X_val_list, axis=0)\n",
    "y_val = pd.concat(y_val_list, axis=0)\n",
    "X_test = pd.concat(X_test_list, axis=0)\n",
    "y_test = pd.concat(y_test_list, axis=0)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Testing set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'auto' strategy for RandomUnderSampler\n",
    "under = RandomUnderSampler(sampling_strategy='auto')\n",
    "X_resampled, y_resampled = under.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest on the resampled data\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=6, max_features=4, random_state=0)\n",
    "clf_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Perform 10-fold cross-validation on the resampled data\n",
    "cv_scores = cross_val_score(clf_rf, X_resampled, y_resampled, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print out the scores for each fold and average accuracy\n",
    "print(\"Cross-validated accuracy scores for each fold:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation of CV Accuracy:\", cv_scores.std())\n",
    "\n",
    "# Calibrate the classifier on the validation set\n",
    "calibrated_rf = CalibratedClassifierCV(clf_rf, method='sigmoid', cv='prefit')\n",
    "calibrated_rf.fit(X_val, y_val)\n",
    "\n",
    "# Predict on the validation set using the calibrated RF\n",
    "y_pred_val = calibrated_rf.predict(X_val)\n",
    "accuracy_score_val = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Calibrated Random Forest Validation accuracy:\", accuracy_score_val)\n",
    "\n",
    "# Predict on the test set using the calibrated RF\n",
    "y_pred_calibrated = calibrated_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_calibrated)\n",
    "print(\"Calibrated Random Forest Test accuracy:\", accuracy_rf)\n",
    "print(\"Calibrated Random Forest Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_calibrated))\n",
    "\n",
    "# Compute metrics for model_metrics dictionary\n",
    "precision_rf, recall_rf, f1_rf, _ = precision_recall_fscore_support(y_test, y_pred_calibrated, average='weighted')\n",
    "model_metrics_rf= {\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": f\"{accuracy_rf * 100:.2f}%\",\n",
    "        \"Precision\": f\"{precision_rf * 100:.2f}%\",\n",
    "        \"Recall\": f\"{recall_rf * 100:.2f}%\",\n",
    "        \"F1 Score\": f\"{f1_rf * 100:.2f}%\"\n",
    "    }\n",
    "}\n",
    "print(\"Model Metrics for Random Forest\", model_metrics_rf)\n",
    "\n",
    "# Compute the confusion matrix for calibrated RF\n",
    "conf_mat_calibrated = confusion_matrix(y_test, y_pred_calibrated)\n",
    "class_labels = np.unique(y_test)\n",
    "\n",
    "# Plot using Seaborn for calibrated RF\n",
    "plt.figure(figsize=(11,7))\n",
    "sns.heatmap(conf_mat_calibrated, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Calibrated Random Forest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) # scale the validation set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Using 'auto' strategy for RandomUnderSampler\n",
    "under = RandomUnderSampler(sampling_strategy='auto')\n",
    "\n",
    "# Apply the undersampling strategy\n",
    "X_train_resampled, y_train_resampled = under.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the K-nearest neighbors (KNN) classifier with adjusted parameters\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5001, weights='distance', algorithm='auto', p=2)\n",
    "\n",
    "# Fit the KNN model with resampled training data\n",
    "clf_knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Perform 10-fold cross-validation on the resampled data\n",
    "cv_scores_knn = cross_val_score(clf_knn, X_train_resampled, y_train_resampled, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print out the scores for each fold and average accuracy\n",
    "print(\"Cross-validated accuracy scores for each fold (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV Accuracy (KNN):\", cv_scores_knn.mean())\n",
    "print(\"Standard Deviation of CV Accuracy (KNN):\", cv_scores_knn.std())\n",
    "\n",
    "# Predict using the KNN model on the scaled validation set\n",
    "y_pred_knn_val = clf_knn.predict(X_val_scaled)\n",
    "accuracy_score_knn_val = accuracy_score(y_val, y_pred_knn_val)\n",
    "print(\"K-nearest Neighbors Validation accuracy:\", accuracy_score_knn_val)\n",
    "\n",
    "# Predict using the KNN model on the scaled test set\n",
    "y_pred_knn = clf_knn.predict(X_test_scaled)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"K-nearest Neighbors Test accuracy:\", accuracy_knn)\n",
    "print(\"K-nearest Neighbors Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Compute metrics for model_metrics dictionary\n",
    "precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn, average='weighted')\n",
    "model_metrics_knn = {\n",
    "    \"Accuracy\": f\"{accuracy_knn * 100:.2f}%\",\n",
    "    \"Precision\": f\"{precision_knn * 100:.2f}%\",\n",
    "    \"Recall\": f\"{recall_knn * 100:.2f}%\",\n",
    "    \"F1 Score\": f\"{f1_knn * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "# Compute the confusion matrix for test set\n",
    "conf_mat_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# Use the previously established method to extract unique class labels\n",
    "class_labels = np.unique(y_test)\n",
    "\n",
    "# Plot using Seaborn\n",
    "plt.figure(figsize=(11,7))\n",
    "sns.heatmap(conf_mat_knn, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for K-nearest Neighbors on Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the RandomUnderSampler\n",
    "under = RandomUnderSampler(sampling_strategy='auto')\n",
    "X_train_under, y_train_under = under.fit_resample(X_train, y_train)\n",
    "\n",
    "# Set var_smoothing parameter for regularization\n",
    "var_smoothing_value = 1e-1\n",
    "clf_nb = GaussianNB(var_smoothing=var_smoothing_value)\n",
    "\n",
    "# Perform 10-fold cross-validation on undersampled data\n",
    "cv_scores = cross_val_score(clf_nb, X_train_under, y_train_under, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print out the scores for each fold and average accuracy\n",
    "print(\"Cross-validated accuracy scores for each fold:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation of CV Accuracy:\", cv_scores.std())\n",
    "\n",
    "# Fit the clf_nb with the undersampled training data\n",
    "clf_nb.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Calibrate the classifier\n",
    "calibrated_nb = CalibratedClassifierCV(clf_nb, method='sigmoid', cv='prefit')\n",
    "calibrated_nb.fit(X_val, y_val)\n",
    "\n",
    "# Predict for the training set using the calibrated model\n",
    "y_pred_nb_train = calibrated_nb.predict(X_train)\n",
    "accuracy_score_nb_train = accuracy_score(y_train, y_pred_nb_train)\n",
    "\n",
    "# Predict for the validation set using the calibrated model\n",
    "y_pred_nb_val = calibrated_nb.predict(X_val)\n",
    "accuracy_score_nb_val = accuracy_score(y_val, y_pred_nb_val)\n",
    "\n",
    "# Predict for the test set using the calibrated model\n",
    "y_pred_nb = calibrated_nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Naive Bayes Validation accuracy:\", accuracy_score_nb_val)\n",
    "print(\"Naive Bayes Test accuracy:\", accuracy_nb)\n",
    "print(\"Naive Bayes Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Compute metrics for model_metrics dictionary\n",
    "precision_nb, recall_nb, f1_nb, _ = precision_recall_fscore_support(y_test, y_pred_nb, average='weighted')\n",
    "model_metrics_nb = {\n",
    "    \"Accuracy\": f\"{accuracy_nb * 100:.2f}%\",\n",
    "    \"Precision\": f\"{precision_nb * 100:.2f}%\",\n",
    "    \"Recall\": f\"{recall_nb * 100:.2f}%\",\n",
    "    \"F1 Score\": f\"{f1_nb * 100:.2f}%\"\n",
    "}\n",
    "# Compute the confusion matrix for the test set\n",
    "conf_mat_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "# Extract unique class labels\n",
    "class_labels = np.unique(y)\n",
    "\n",
    "# Plot using Seaborn for the test set\n",
    "plt.figure(figsize=(11,7))\n",
    "sns.heatmap(conf_mat_nb, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Naive Bayes on Test Set')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression with stronger regularization (smaller C value)\n",
    "clf_lr = LogisticRegression(C=0.001, max_iter=5000, random_state=0)\n",
    "clf_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calibrating the classifier using the previously fitted instance\n",
    "calibrated_clf = CalibratedClassifierCV(clf_lr, method='sigmoid', cv='prefit')\n",
    "calibrated_clf.fit(X_val_scaled, y_val)\n",
    "\n",
    "# Perform 10-fold cross-validation on the training data with original classifier\n",
    "cv_scores = cross_val_score(clf_lr, X_train_scaled, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print out the scores for each fold and average accuracy\n",
    "print(\"Cross-validated accuracy scores for each fold:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation of CV Accuracy:\", cv_scores.std())\n",
    "\n",
    "# Predict on the validation set using calibrated classifier\n",
    "y_pred_lr_val = calibrated_clf.predict(X_val_scaled)\n",
    "accuracy_score_lr_val = accuracy_score(y_val, y_pred_lr_val)\n",
    "print(\"Logistic Regression Validation accuracy:\", accuracy_score_lr_val)\n",
    "\n",
    "# Predict on the test set using calibrated classifier\n",
    "y_pred_lr = calibrated_clf.predict(X_test_scaled)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print the Logistic Regression Classification Report for the test set\n",
    "print(\"Logistic Regression Test accuracy:\", accuracy_lr)\n",
    "print(\"Logistic Regression Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Compute metrics for model_metrics dictionary\n",
    "precision_lr, recall_lr, f1_lr, _ = precision_recall_fscore_support(y_test, y_pred_lr, average='weighted')\n",
    "model_metrics_lr = {\n",
    "    \"Accuracy\": f\"{accuracy_lr * 100:.2f}%\",\n",
    "    \"Precision\": f\"{precision_lr * 100:.2f}%\",\n",
    "    \"Recall\": f\"{recall_lr * 100:.2f}%\",\n",
    "    \"F1 Score\": f\"{f1_lr * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "# Compute the confusion matrix for the test set\n",
    "conf_mat_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Extract unique class labels\n",
    "class_labels = np.unique(y)\n",
    "\n",
    "# Plot using Seaborn for the test set\n",
    "plt.figure(figsize=(11,7))\n",
    "sns.heatmap(conf_mat_lr, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Logistic Regression on Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "clf_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=5,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.01,\n",
    "    learning_rate=0.03,\n",
    "    objective='multi:softmax',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Perform 10-fold cross-validation on the original XGBClassifier\n",
    "cv_scores = cross_val_score(clf_xgb, X_train, y_train_encoded, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print out the scores for each fold and average accuracy\n",
    "print(\"Cross-validated accuracy scores for each fold:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "print(\"Standard Deviation of CV Accuracy:\", cv_scores.std())\n",
    "\n",
    "# Calibrate the classifier AFTER the cross-validation\n",
    "calibrated_xgb = CalibratedClassifierCV(clf_xgb, method='sigmoid', cv='prefit')\n",
    "calibrated_xgb.fit(X_val, y_val_encoded)\n",
    "\n",
    "# Predict on the validation set using the calibrated classifier\n",
    "y_pred_xgb_val = le.inverse_transform(calibrated_xgb.predict(X_val))\n",
    "\n",
    "# Compute the accuracy for the validation set\n",
    "accuracy_score_xgb_val = accuracy_score(y_val, y_pred_xgb_val)\n",
    "print(\"Calibrated XGBoost Validation accuracy:\", accuracy_score_xgb_val)\n",
    "\n",
    "# Decode the predicted labels to original string format for the test set\n",
    "y_pred_xgb = le.inverse_transform(calibrated_xgb.predict(X_test))\n",
    "\n",
    "# Use the decoded predicted labels for calculating accuracy and plotting confusion matrix\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Test accuracy:\", accuracy_xgb)\n",
    "print(\"XGBoost Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Extract unique class labels from y_train\n",
    "class_labels = np.unique(y_train)\n",
    "\n",
    "# Compute metrics for model_metrics dictionary\n",
    "precision_xgb, recall_xgb, f1_xgb, _ = precision_recall_fscore_support(y_test, y_pred_xgb, average='weighted')\n",
    "model_metrics_xgb = {\n",
    "    \"Accuracy\": f\"{accuracy_xgb * 100:.2f}%\",\n",
    "    \"Precision\": f\"{precision_xgb * 100:.2f}%\",\n",
    "    \"Recall\": f\"{recall_xgb * 100:.2f}%\",\n",
    "    \"F1 Score\": f\"{f1_xgb * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_mat_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "# Plot using Seaborn\n",
    "plt.figure(figsize=(11,7))\n",
    "sns.heatmap(conf_mat_xgb, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Calibrated XGBoost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute metrics\n",
    "metrics = {\n",
    "    \"Model\": [\"Random Forest\", \"KNN\", \"Naive Bayes\", \"Logistic Regression\", \"XGBoost\"],\n",
    "    \"Accuracy\": [accuracy_rf, accuracy_knn, accuracy_nb, accuracy_lr, accuracy_xgb],\n",
    "    \"Precision\": [precision_score(y_test, y_pred_calibrated, average='weighted'),\n",
    "                  precision_score(y_test, y_pred_knn, average='weighted'),\n",
    "                  precision_score(y_test, y_pred_nb, average='weighted'),\n",
    "                  precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "                  precision_score(y_test, y_pred_xgb, average='weighted')],\n",
    "    \"Recall\": [recall_score(y_test, y_pred_calibrated, average='weighted'),\n",
    "               recall_score(y_test, y_pred_knn, average='weighted'),\n",
    "               recall_score(y_test, y_pred_nb, average='weighted'),\n",
    "               recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "               recall_score(y_test, y_pred_xgb, average='weighted')],\n",
    "    \"F1 Score\": [f1_score(y_test, y_pred_calibrated, average='weighted'),\n",
    "                 f1_score(y_test, y_pred_knn, average='weighted'),\n",
    "                 f1_score(y_test, y_pred_nb, average='weighted'),\n",
    "                 f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "                 f1_score(y_test, y_pred_xgb, average='weighted')]\n",
    "}\n",
    "\n",
    "# Print in tabular form\n",
    "print(\"{:<20} {:<10} {:<10} {:<10} {:<10}\".format('Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'))\n",
    "print(\"-\"*60)  # print separator line\n",
    "for model, acc, prec, rec, f1 in zip(metrics[\"Model\"], metrics[\"Accuracy\"], metrics[\"Precision\"], metrics[\"Recall\"], metrics[\"F1 Score\"]):\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(model, acc, prec, rec, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Random Forest', 'KNN', 'Naive Bayes', 'Logistic Regression', 'XGBoost']\n",
    "accuracies = [accuracy_rf, accuracy_knn, accuracy_nb, accuracy_lr, accuracy_xgb] \n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "# Using consistent colors with a slight alpha transparency and edge color\n",
    "bars = ax.bar(model_names, accuracies, color='skyblue', alpha=0.75, edgecolor='black')\n",
    "\n",
    "# Adding data labels with 4 decimal places\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, \"{:.5f}\".format(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold', labelpad=10)  # labelpad adds some padding to the ylabel\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim(0, 1.125)  # Added a bit more space at the top for the data labels\n",
    "ax.set_yticks([i/10 for i in range(11)])\n",
    "ax.set_yticklabels([i/10 for i in range(11)], fontsize=10)\n",
    "ax.set_xticks(range(len(model_names)))\n",
    "\n",
    "# Adjusting font size of the classifier names\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=10)  # Reduced font size to 10\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)  # Adding horizontal grid lines\n",
    "\n",
    "# Adjust the position of the axes to create margins inside the chart\n",
    "ax.set_position([0.5, 0.5, 0.7, 0.7])  # left, bottom, width, height\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = float(input(\"Enter the value of your N: \"))\n",
    "P = float(input(\"Enter the value of your P: \"))\n",
    "K = float(input(\"Enter the value of your K: \"))\n",
    "temperature = float(input(\"Enter the value of your Temperature (C): \"))\n",
    "humidity = float(input(\"Enter the value of your Humidity: \"))\n",
    "ph = float(input(\"Enter the value of your Ph: \"))\n",
    "rainfall = float(input(\"Enter the value of your Rainfall: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define original feature names and input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_names = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "data = np.array([[25, 51, 18, 27.77799528, 54.82130787, 9.45949344, 50.28438729]])\n",
    "data_df = pd.DataFrame(data, columns=original_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Random Forest\": clf_rf,\n",
    "    \"KNN\": clf_knn,\n",
    "    \"Naive Bayes\": clf_nb,\n",
    "    \"Logistic Regression\": clf_lr,\n",
    "    \"XGBoost\": clf_xgb,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Prediction Function**\n",
    "<br>We'll encapsulate the ensemble prediction logic into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(classifiers, data_df, le=None):\n",
    "    # Convert data_df to 2D if it's 1D\n",
    "    if len(data_df.shape) == 1:\n",
    "        data_df = data_df.values.reshape(1, -1)\n",
    "    else:\n",
    "        data_df = data_df.values\n",
    "        \n",
    "    votes = []\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        proba = clf.predict_proba(data_df)\n",
    "        predicted_index = np.argmax(proba)\n",
    "        \n",
    "        if clf_name == \"XGBoost\" and le:\n",
    "            predicted_label = le.inverse_transform([predicted_index])[0]\n",
    "        else:\n",
    "            predicted_label = clf.classes_[predicted_index]\n",
    "\n",
    "        votes.append(predicted_label)\n",
    "    \n",
    "    vote_counts = Counter(votes)\n",
    "    most_common_votes = vote_counts.most_common()\n",
    "    if len(most_common_votes) > 1 and most_common_votes[0][1] == most_common_votes[1][1]:\n",
    "        tied_classes = [vote[0] for vote in most_common_votes if vote[1] == most_common_votes[0][1]]\n",
    "        return tied_classes[0]\n",
    "    else:\n",
    "        return most_common_votes[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Metrics Calculation**\n",
    "<br>\n",
    "This is where we'll genuinely calculate the ensemble metrics by using the hard voting mechanism on a separate test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ensemble_metrics(classifiers, X_test, y_test, le=None):\n",
    "    y_preds = [ensemble_predict(classifiers, pd.Series(row), le) for row in X_test.values]\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_preds),\n",
    "        'Precision': precision_score(y_test, y_preds, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_preds, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, y_preds, average='weighted')\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization and Outputs**\n",
    "<br>\n",
    "Here we visualize the ensemble predictions and show the results in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ensemble prediction function\n",
    "ensemble_prediction = ensemble_predict(classifiers, data_df, le)\n",
    "\n",
    "# Getting metrics using the function (assuming you have X_test and y_test)\n",
    "ensemble_metrics = calculate_ensemble_metrics(classifiers, X_test, y_test, le)\n",
    "\n",
    "# Tabular Representation of Model-wise Predictions\n",
    "print(\"Model-wise Predictions:\")\n",
    "print(\"| Model Name            | Prediction   |\")\n",
    "print(\"|-----------------------|--------------|\")\n",
    "\n",
    "votes = []  # List to collect individual model predictions\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    proba = clf.predict_proba(data_df.values)\n",
    "    predicted_index = np.argmax(proba)\n",
    "    \n",
    "    if clf_name == \"XGBoost\" and le:\n",
    "        predicted_label = le.inverse_transform([predicted_index])[0]\n",
    "    else:\n",
    "        predicted_label = clf.classes_[predicted_index]\n",
    "    print(f\"| {clf_name:<21} | {predicted_label:<12} |\")\n",
    "    \n",
    "    votes.append(predicted_label)  # Add the model's prediction to the votes list\n",
    "\n",
    "# Visualization: Histogram of Predictions\n",
    "vote_counts = Counter(votes)\n",
    "plt.figure(figsize=(10,6))\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'purple']\n",
    "plt.bar(vote_counts.keys(), vote_counts.values(), color=colors, alpha=0.75)\n",
    "plt.yticks(np.arange(0, 6, 1))\n",
    "plt.ylim(0, 5)\n",
    "plt.ylabel('Number of Models Voting', fontsize=12)\n",
    "plt.xlabel('Crops', fontsize=12)\n",
    "plt.title('Number of Model Votes for Each Predicted Crop', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.margins(0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabular Representation of Ensemble Prediction Results\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(\"              ENSEMBLE PREDICTION RESULT      \")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"| Crop Predicted   | {ensemble_prediction:<27} |\")\n",
    "print(\"|------------------|-----------------------------|\")\n",
    "print(f\"| Accuracy         | {ensemble_metrics['Accuracy']:<27} |\")\n",
    "print(f\"| Precision        | {ensemble_metrics['Precision']:<27} |\")\n",
    "print(f\"| Recall           | {ensemble_metrics['Recall']:<27} |\")\n",
    "print(f\"| F1 Score         | {ensemble_metrics['F1 Score']:<27} |\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\\nNote:\")\n",
    "print(\"The ensemble prediction is derived from a majority consensus across various models.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
